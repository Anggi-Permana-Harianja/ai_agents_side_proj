{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f996024-7822-4263-b64b-6484c0db2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9555a2-a3d1-41f7-95d2-22f81c9b44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "env_ = load_dotenv(find_dotenv())\n",
    "print(env_)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed2b96-77c9-450f-8ab4-f4235098033a",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1f40c-ffc7-4d3a-a126-68db452de03a",
   "metadata": {},
   "source": [
    "# Model. prompt, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966dc0f3-138b-4026-ab50-e3441fdaf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple OpenAI chatGPT\n",
    "\"\"\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda484d5-0a08-45bc-83b6-de511719ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=LLM_MODEL):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\":prompt\n",
    "    }]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4c7696-3613-443a-9b40-3f164a1e4ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"what is 1 + 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2085b2e-e087-41af-b3d7-8d51cd3031e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_1405/4057254475.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(temperature=0, model=LLM_MODEL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x105e23ad0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x105e994c0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple langchain\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe8963b-1ff2-4c81-8bb1-a30c38c86f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e785ea-d411-43a4-8d65-21b8edbca6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50923733-050b-4720-a01e-bfba67f3f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d43989-f6b4-483d-9520-0d8e4b5731ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style,\n",
    "    text=customer_email\n",
    ")\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275be1e2-de37-4e03-9a19-b34c915d51a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_1405/661283754.py:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  customer_response = chat(customer_messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would really appreciate your assistance with this issue. Thank you.\n"
     ]
    }
   ],
   "source": [
    "# the output\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca42eb7-7fb0-4e2c-bb24-533986c40f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Langchain parsing\n",
    "\"\"\"\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was the item purchased \\\n",
    "        as a gift for someone else? \\\n",
    "        Answer True if yes \\\n",
    "        False if not unknown\"\n",
    ")\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days \\\n",
    "        did it take for this product \\\n",
    "        to arrive?, if this information \\\n",
    "        not found, output -1\"\n",
    ")\n",
    "price_value_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any\\\n",
    "        sentences about value\\\n",
    "        or price, and output them as\\\n",
    "        a comma separated Python list\"\n",
    ")\n",
    "\n",
    "response_schemas=[gift_schema, delivery_days_schema, price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89abd5c7-c775-46ab-86de-a5bf44c68eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas=response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39991fd5-b29a-4b21-8235-95537cfc15c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='gift', description='Was the item purchased         as a gift for someone else?         Answer True if yes         False if not unknown', type='string'), ResponseSchema(name='delivery_days', description='How many days         did it take for this product         to arrive?, if this information         not found, output -1', type='string'), ResponseSchema(name='price_value', description='Extract any        sentences about value        or price, and output them as        a comma separated Python list', type='string')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af54f28-4343-4a3f-bae4-b9511d85baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased         as a gift for someone else?         Answer True if yes         False if not unknown\n",
      "\t\"delivery_days\": string  // How many days         did it take for this product         to arrive?, if this information         not found, output -1\n",
      "\t\"price_value\": string  // Extract any        sentences about value        or price, and output them as        a comma separated Python list\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "set up instructions\n",
    "\"\"\"\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9497783d-cb7f-421c-a227-b4952757bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parsing review\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=review_template\n",
    ")\n",
    "messages = prompt.format_messages(\n",
    "    text=customer_review,\n",
    "    format_instructions=format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fb0ec0-1fcf-4e3b-8344-d4435017f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": \"True\",\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f5769b-66e2-49b2-9c71-caefce4e1633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ipmortant: as you may see that the parsed is still a STR instead of DICT\n",
    "\"\"\"\n",
    "print(type(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce5ea66-7ecf-4109-8fce-907dd8140f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse into DICT\n",
    "\"\"\"\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9435d674-b400-4382-b32a-be8cdd5ddd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': 'True',\n",
       " 'delivery_days': '2',\n",
       " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "907d4faa-1698-4ad9-b7b9-4c05ee03b8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ede2158-c63f-482a-8e4e-490faaa53db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bf3d940-d499-4171-8009-d7c83ca474e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('price_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2d83b-96ca-4e53-913c-1931137d41b0",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d1832-73bf-4eb9-ba8c-7ca9cf8f954e",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006e2a1-45fb-45ad-b439-1d364d8d2ec3",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory\n",
    "\n",
    "### What it does\n",
    "\n",
    "- Stores all messages (both user and AI) in a growing buffer.\n",
    "- Appends each new message to the buffer.\n",
    "\n",
    "### Use case\n",
    "\n",
    "- Useful when you want to keep the entire conversation context available.\n",
    "- Good for shorter conversations or scenarios where you really need the entire chat history.\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "- Pro: Straightforward; you always have the full conversation.\n",
    "- Con: For longer conversations, the prompt can get very large (and potentially exceed token limits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cdd4417-4fe3-46b4-9f2a-460c879abc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_1405/2089055330.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_1405/2089055330.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You only need a arguments into conversation chain\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3eaf6857-b438-4f2e-baf8-0106693f0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My name is Anggi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Anggi\n",
      "AI: Hello, Anggi! It's great to meet you! How's your day going so far?\n",
      "Human: what is 1 + 1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: My name is Anggi\n",
      "AI: Hello, Anggi! It's great to meet you! How's your day going so far?\n",
      "Human: what is 1 + 1?\n",
      "AI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\n",
      "Human: whats my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Anggi! It's nice to remember it. If there's anything else you'd like to chat about or ask, I'm here for you!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of LLM that has memory\n",
    "\"\"\"\n",
    "conversation.predict(input=\"My name is Anggi\")\n",
    "conversation.predict(input=\"what is 1 + 1?\")\n",
    "conversation.predict(input=\"whats my name again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09726923-39fc-4707-ae02-1dccd628a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is Anggi\n",
      "AI: Hello, Anggi! It's great to meet you! How's your day going so far?\n",
      "Human: what is 1 + 1?\n",
      "AI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\n",
      "Human: whats my name again?\n",
      "AI: Your name is Anggi! It's nice to remember it. If there's anything else you'd like to chat about or ask, I'm here for you!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c625f1db-e3d5-4c44-8826-3e2f056153d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: My name is Anggi\\nAI: Hello, Anggi! It's great to meet you! How's your day going so far?\\nHuman: what is 1 + 1?\\nAI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\\nHuman: whats my name again?\\nAI: Your name is Anggi! It's nice to remember it. If there's anything else you'd like to chat about or ask, I'm here for you!\"}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5574d07-3b70-4654-a12b-cdf5b31b0898",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Also stores messages in a buffer, but only up to a certain number of most recent turns.\n",
    "- For example, you can set k=3 to store the last 3 exchanges.\n",
    "\n",
    "#### Use case\n",
    "\n",
    "- Useful when you only need recent context (e.g., the last 5 messages) without carrying the entire history.\n",
    "- Helps prevent hitting token limits with very long chats.\n",
    "\n",
    "#### Pros & Cons\n",
    "\n",
    "- Pro: You get a rolling “window” of recent conversation, keeping context relevant without enormous prompts.\n",
    "- Con: Older context is lost if it’s not within the last k turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dc81e4e-cf01-4a7d-a62c-5ea297878efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_1405/1375867408.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f823ea5f-d377-4182-b35e-9e6dede39c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Anggi! It's great to meet you! How's your day going so far?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "since we set k=1, only 1 previous chat will be stored in memory \n",
    "\"\"\"\n",
    "conversation.predict(input=\"Hi, my name is Anggi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cbfc8c4-0b35-42bc-85f6-372ba3a8f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1 + 1 equals 2! It's one of the simplest math problems, but it's also the foundation for so many other concepts in mathematics. Do you enjoy math, or is there another subject you prefer?\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a43777-5525-49db-8db2-fd3e0fff9466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m sorry, but I don’t know your name! I don’t have access to personal information unless you share it with me. If you’d like, you can tell me your name, and I’ll remember it for our conversation! What do you like to be called?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "the LLM should be forgot my name now\n",
    "\"\"\"\n",
    "conversation.predict(input=\"what is my name again?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321ee4b-4ea5-4209-a384-fa8179abf71c",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Similar idea to “windowed” memory, but it’s token-based instead of message-based.\n",
    "\n",
    "- Maintains conversation history until a maximum token count is reached.\n",
    "\n",
    "- If adding a new message would exceed the token limit, older messages are dropped.\n",
    "\n",
    "#### Use case\n",
    "\n",
    "- More fine-grained control than ConversationBufferWindowMemory because it accounts for actual token usage, not just the number of messages.\n",
    "\n",
    "- Useful in apps that are especially sensitive to token costs or API token limits.\n",
    "\n",
    "#### Pros & Cons\n",
    "\n",
    "- Pro: You can precisely manage token usage, ensuring you don’t exceed a certain token budget.\n",
    "\n",
    "- Con: Less predictable in how many messages remain in memory (depends on message lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f583152d-31ea-41e4-839d-7cd361a2778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token=3\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "768137fe-7cac-4247-bad7-f07fd7f73018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The LLM should only remeber at max 50 tokens\n",
    "\"\"\"\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87cb3ea0-8426-4ca8-b124-844baca1062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: give me 51 tokens\\nAI: Sure! Here are 51 tokens for you: \\n\\n1. Token 1\\n2. Token 2\\n3. Token 3\\n4. Token 4\\n5. Token 5\\n6. Token 6\\n7. Token 7\\n8. Token 8\\n9. Token 9\\n10. Token 10\\n11. Token 11\\n12. Token 12\\n13. Token 13\\n14. Token 14\\n15. Token 15\\n16. Token 16\\n17. Token 17\\n18. Token 18\\n19. Token 19\\n20. Token 20\\n21. Token 21\\n22. Token 22\\n23. Token 23\\n24. Token 24\\n25. Token 25\\n26. Token 26\\n27. Token 27\\n28. Token 28\\n29. Token 29\\n30. Token 30\\n31. Token 31\\n32. Token 32\\n33. Token 33\\n34. Token 34\\n35. Token 35\\n36. Token 36\\n37. Token 37\\n38. Token 38\\n39. Token 39\\n40. Token 40\\n41. Token 41\\n42. Token 42\\n43. Token 43\\n44. Token 44\\n45. Token 45\\n46. Token 46\\n47. Token 47\\n48. Token 48\\n49. Token 49\\n50. Token 50\\n51. Token 51\\n\\nLet me know if you need anything else!\\nHuman: AI is what?!\\nAI: Amazing!\\nHuman: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccd617-b661-465d-ac24-83c4cdad9030",
   "metadata": {},
   "source": [
    "### ConversationSummary\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Rather than keeping every message, this memory periodically creates a summary (or “compressed history”) of the conversation so far.\n",
    "\n",
    "- When new messages come in, it updates the summary using an LLM to include the important points.\n",
    "\n",
    "#### Use case\n",
    "\n",
    "- Ideal for long conversations where you want to retain key context without blowing up token usage.\n",
    "\n",
    "- The chain references the evolving summary plus the most recent messages.\n",
    "\n",
    "#### Pros & Cons\n",
    "\n",
    "- Pro: You keep the conversation “essence” over many turns while controlling prompt size.\n",
    "\n",
    "- Con: Summaries are approximations; some nuance from earlier turns can be lost if the summary isn’t perfectly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e082d5b4-56a0-45dc-a941-15542f5a25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, \n",
    "    max_token_limit=100\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8646717f-1c3c-4226-91d5-953da2ead1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97bf3448-4c03-40ab-9204-c83a04b0aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"what is on the schedule?\"},\n",
    "    {\"output\": f\"{schedule}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "621c2e33-75b2-4327-86d7-22611ee7c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human asks what is on the schedule.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A great demo to show would be a conversational AI application that utilizes a large language model (LLM) to answer questions in real-time. You could set up a scenario where the AI assists with common customer inquiries, showcasing its ability to understand context and provide relevant information. \\n\\nAnother option could be demonstrating a text generation feature, where you input a prompt and the AI generates a coherent and contextually appropriate response. This could highlight the model's creativity and versatility. \\n\\nIf you have access to any recent advancements in AI, such as fine-tuning the model for specific tasks or integrating it with other tools, that could also be impressive to showcase. Just make sure to tailor the demo to the interests of your customer, focusing on how these capabilities can benefit their business!\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "it will summary what written in schedule\n",
    "\"\"\"\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b9cc8a4-1662-4532-9c1a-977a9d660280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human asks what is on the schedule. The AI outlines a meeting at 8am with the product team, time to work on the LangChain project from 9am to 12pm, and a lunch meeting at noon with a customer to discuss the latest in AI. The AI suggests bringing a laptop to show a demo of a large language model (LLM). The human inquires about a good demo to show, and the AI recommends demonstrating a conversational AI application that answers customer inquiries in real-time or showcasing a text generation feature. The AI emphasizes tailoring the demo to the customer's interests and highlighting how these AI capabilities can benefit their business.\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cfcea-bffb-4962-bfd1-375831320571",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd428-7915-4aab-b27f-f191c94264e0",
   "metadata": {},
   "source": [
    "## Chains in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df83728-a416-4e7b-a1f1-32a9f0552ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce7107d-0d09-492e-8c76-c9ed65aabe36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15703de9-8a87-4cb9-ac5c-e234d53635f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Review[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd8e1d8-0956-40fe-99b5-279a6bc33c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=LLM_MODEL)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6da9b5-d6fb-4ac2-bacd-c8e714daf304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_2665/1305865249.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1168d18f-fcc5-42df-a88b-2ca53f127ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_2665/550859008.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(product)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Coming up with a catchy and descriptive name for a company that specializes in queen-size sheet sets can help attract customers and convey the brand's essence. Here are some suggestions:\\n\\n1. **Queen Fit Sheets**\\n2. **RoyalRest Linens**\\n3. **Majestic Queen Sheets**\\n4. **Queen Comfort Co.**\\n5. **Dreamy Queen Sets**\\n6. **Sovereign Sleep Sheets**\\n7. **Regal Dreams Bedding**\\n8. **Queen's Touch Textiles**\\n9. **Elegant Queen Essentials**\\n10. **NobleNest Linens**\\n\\nFeel free to mix and match or modify these suggestions to find the perfect name that resonates with your brand vision!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094a31f-b756-44bc-96ee-e6cde4aab1a5",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain (one input, one output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f098e02-8041-4ffb-bad4-92fd7cc9a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cf2d9fa-b4ae-4db1-8b59-00920e083f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "first chain\n",
    "\"\"\"\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"what is the best name to describe a company that makes {product}\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbcf5d2e-bdb8-4b5a-964e-5b11cde7013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "second chain\n",
    "\"\"\"\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following company: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "785aae9e-d47b-457c-a9e3-aa122649476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chains on first and second\n",
    "\"\"\"\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eef01bc7-6da0-43c2-bb9f-6e6ae89a6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mChoosing a name for a company that specializes in queen size sheet sets can reflect comfort, quality, and style. Here are some suggestions:\n",
      "\n",
      "1. **Queen's Comfort**\n",
      "2. **Royal Rest Sheets**\n",
      "3. **Serene Sleep Co.**\n",
      "4. **Majestic Bedding**\n",
      "5. **Dreamy Queen**\n",
      "6. **Plush Queen Sheets**\n",
      "7. **LuxeSleep Collections**\n",
      "8. **Queen Haven Linens**\n",
      "9. **Slumber Majesty**\n",
      "10. **Regal Sleep Essentials**\n",
      "\n",
      "Consider how the name aligns with your brand's identity and target market!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mExperience ultimate comfort and style with our queen sheet sets, designed for serene sleep and luxurious relaxation. Quality you’ll love!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Experience ultimate comfort and style with our queen sheet sets, designed for serene sleep and luxurious relaxation. Quality you’ll love!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3b40c-e668-486d-86e5-4d510d306a98",
   "metadata": {},
   "source": [
    "### SequentialChain (multiple input, multiple outout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "180b4fc2-7dff-4000-bb88-16b63c264452",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe90a4c9-91c6-4334-aa13-b7fc99b46484",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review into English: {Review}\"\n",
    ")\n",
    "chain_one = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=first_prompt,\n",
    "    output_key=\"English_Review\" #Please make sure all the key are match to be chained\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "449ea1f8-8ca2-45e0-bb9f-0635abea5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence: {English_Review}\"\n",
    ")\n",
    "chain_two = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt,\n",
    "    output_key=\"summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f87d3b8-3364-49a5-a4c3-a3205c05444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review: {Review}\"\n",
    ")\n",
    "chain_three = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=third_prompt,\n",
    "    output_key=\"language\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c3de346-c648-40ba-b5be-6380de4e685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following\"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\nSummary: {summary}\\nLanguage: {language}\"\n",
    ")\n",
    "chain_four = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=fourth_prompt,\n",
    "    output_key=\"followup_message\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8f5c308-6dbb-4b15-b46e-0e44be58a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Overall chain, \n",
    "make sure all the output_key are match\n",
    "\"\"\"\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"language\", \"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "229bfb32-50ea-4652-8d11-373e9d292ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_2665/2952226717.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_chain(review)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': 'I find the taste mediocre. The mousse doesn’t hold, which is strange. I buy the same ones in stores and the taste is much better...  \\nOld batch or counterfeit!?',\n",
       " 'summary': \"The mousse tasted mediocre and didn't hold properly, leading to suspicions of it being an old batch or counterfeit, unlike the better quality ones bought in stores.\",\n",
       " 'language': 'The review is written in French.',\n",
       " 'followup_message': \"Il est décevant d'apprendre que la mousse n'a pas été à la hauteur de vos attentes. Les problèmes de texture et de goût peuvent effectivement soulever des questions sur la qualité du produit. Avez-vous pensé à contacter le fabricant ou le point de vente pour signaler votre expérience ? Ils pourraient être intéressés par vos retours et éventuellement vous proposer un échange ou un remboursement. Il est toujours bon de partager ces retours pour améliorer la qualité des produits disponibles.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run the chain\n",
    "\"\"\"\n",
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee07d1c-0a64-480c-8864-f12ff87bcfee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ccce4c2-0107-44ac-839b-fd3182ea49bc",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee2c8a-4e71-43f3-9c6a-3e465fbe1c98",
   "metadata": {},
   "source": [
    "### RouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "290f5892-a9fc-45f7-9768-928b1fe92f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Templates\n",
    "\"\"\"\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e868c37-7b8a-43fb-9a0d-3a8f5b196b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prompt dictionary\n",
    "\"\"\"\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a450ca92-6020-4917-8772-a5436b359608",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41b6ca33-bfcf-42f4-a93c-0a3c66749b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Destination chains\n",
    "\"\"\"\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=prompt_template\n",
    "    )\n",
    "    chain = LLMChain(\n",
    "        llm=llm, \n",
    "        prompt=prompt\n",
    "    )\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fd224fc-a0ed-4d3e-a248-7cbbe15350b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5c40e0a-137e-48ed-adbf-cdbea33f365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for the default format, if none of the context mentioned in template\n",
    "\"\"\"\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6712f412-fdce-4295-b283-4ad84e22daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_2665/2160379643.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Router template, \n",
    "make sure to have the <<INPUT>>, <<OUTPUT>> template\n",
    "\"\"\"\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4bad0ac-0822-490a-bbe7-a7f9390ea88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "physics: Good for answering questions about physics\n",
      "math: Good for answering math questions\n",
      "History: Good for answering history questions\n",
      "computer science: Good for answering computer science questions\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\n"
     ]
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "848960fc-2fc5-4134-98c9-7ced33a0564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Router template\n",
    "\"\"\"\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm, \n",
    "    router_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be5749d8-71f5-482e-bb89-9a536f0eb887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/c92r7j0x63v8v0t9zqv4cxpr0000gn/T/ipykernel_2665/749833255.py:4: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chain MultiPrompt\n",
    "\"\"\"\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eee8d9a8-5e89-4a94-aec4-bebda2c720cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black body radiation refers to the electromagnetic radiation emitted by a theoretical perfect absorber and emitter of all frequencies of radiation, known as a \"black body.\" In physics, a black body is an idealized object that does not reflect any light and absorbs all incident radiation, regardless of frequency or angle of incidence.\\n\\nWhen a black body is heated, it emits radiation in a characteristic spectrum that depends only on its temperature. This radiation can be described by Planck\\'s Law, which shows that the intensity of radiation emitted at different wavelengths peaks at a specific wavelength inversely related to the temperature (Wien\\'s Displacement Law). At higher temperatures, the emitted radiation shifts toward shorter wavelengths, ranging from infrared at lower temperatures to visible light and even ultraviolet at higher temperatures.\\n\\nThe study of black body radiation was crucial in the development of quantum mechanics, particularly because classical physics could not explain the observed spectrum of emitted radiation, leading to the so-called \"ultraviolet catastrophe.\" Max Planck\\'s introduction of quantized energy levels helped resolve this issue and laid the groundwork for modern physics.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b878702-5676-4e64-ba08-49710a1a64af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To solve the question \\\\(2 + 2\\\\), we can break it down into its components:\\n\\n1. Identify the two numbers we are adding: \\\\(2\\\\) and \\\\(2\\\\).\\n2. Understand that addition involves combining these two numbers.\\n\\nNow, we perform the addition:\\n\\n\\\\[\\n2 + 2 = 4\\n\\\\]\\n\\nSo the answer to the question is \\\\(4\\\\).'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dded1ebf-3b52-404a-8784-1d54a974d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in the human body contains DNA because DNA carries the genetic instructions necessary for the growth, development, functioning, and reproduction of all living organisms. Here are several key reasons why all cells contain DNA:\\n\\n1. **Genetic Blueprint**: DNA serves as the blueprint for the synthesis of proteins and other molecules essential for life. Each cell uses its DNA to produce the proteins that are necessary for its specific functions.\\n\\n2. **Cellular Function and Differentiation**: Although different cell types have specialized functions (e.g., muscle cells, nerve cells, skin cells), they all originate from a single fertilized egg (zygote) that divides and differentiates. The DNA in every cell contains the complete genetic information necessary to develop into a whole organism.\\n\\n3. **Replication**: When cells divide, they must replicate their DNA so that each new cell has a complete set of genetic instructions. This ensures that all cells in the body maintain genetic continuity.\\n\\n4. **Stability and Repair**: DNA contains mechanisms for self-repair and maintenance. Cells constantly monitor and repair their DNA to prevent mutations and maintain genetic integrity, which is crucial for long-term health and function.\\n\\n5. **Regulation of Cellular Activity**: The expression of genes in the DNA regulates cellular activities, such as metabolism, response to stimuli, and cell communication. This regulation is essential for the proper functioning of tissues and organs.\\n\\n6. **Inheritance**: DNA carries the hereditary information that is passed from parents to offspring. This genetic continuity is fundamental for the evolution and adaptation of species.\\n\\nIn summary, the presence of DNA in every cell is fundamental to life, as it is the source of genetic information necessary for the proper functioning, growth, and reproduction of organisms.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bae8d386-8c97-4c44-af14-4cbd5c5cf2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'give me the chemical composition of Oxygen'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oxygen is a chemical element with the symbol \"O\" and atomic number 8. It is a diatomic molecule in its most common form, which means it exists as O₂ (two oxygen atoms bonded together). The chemical composition of molecular oxygen is simply O₂. \\n\\nIn terms of its elemental composition, oxygen is composed entirely of oxygen atoms. In its various allotropes, the most significant forms include:\\n\\n1. **O₂ (Dioxygen)**: The most common form of oxygen, essential for respiration in most life forms.\\n2. **O₃ (Ozone)**: A triatomic molecule consisting of three oxygen atoms, found in the Earth\\'s stratosphere, where it plays a crucial role in absorbing UV radiation.\\n\\nIf you are looking for the chemical composition of oxygen in a compound, that would depend on the specific compound (e.g., H₂O for water, CO₂ for carbon dioxide, etc.).'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We try to give no context\n",
    "chain.run(\"give me the chemical composition of Oxygen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df2237-08fe-4e86-8876-481e91d0dfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
